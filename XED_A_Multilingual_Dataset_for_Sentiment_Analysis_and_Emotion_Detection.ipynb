{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XED: A Multilingual Dataset for Sentiment Analysis and Emotion Detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOfDLEf58DiWyCROODi9uhP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/markok20/XED-project/blob/main/XED_A_Multilingual_Dataset_for_Sentiment_Analysis_and_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tZNC_nAshr8"
      },
      "source": [
        "LOAD_FROM_LOCAL = True\n",
        "\n",
        "BASE_URL   = 'https://lthelsinkioffens.slack.com/files/UTPM2EMJS/F016E18GDA5/'\n",
        "TRAIN_PATH = 'NERadx.train'\n",
        "DEV_PATH   = 'NERadx.dev'\n",
        "TEST_PATH  = 'NERadx.test'\n",
        "\n",
        "if LOAD_FROM_LOCAL:\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Wf04pcs947"
      },
      "source": [
        "#@title Hyperparameters\n",
        "\n",
        "SEED = 12345 #@param {type:\"raw\"}\n",
        "\n",
        "BERT_MODEL = 'english_base_cased' #@param [\"multilingual\", \"english_base_cased\", \"english_large_cased\", \"english_base_uncased\", \"english_large_uncased\", \"finnish_cased\", \"finnish_uncased\", \"dutch\", \"chinese\", \"german\", \"arabic\", \"greek\", \"turkish\"]\n",
        "#DO_PREPROCESSING = False #@param {type:\"boolean\"}\n",
        "#DO_BALANCING = False #@param {type:\"boolean\"}\n",
        "\n",
        "EPOCHS = 3 #@param [\"2\", \"3\", \"4\"] {type:\"raw\"}\n",
        "MAX_LEN =  48  #@param [\"32\", \"48\", \"64\", \"128\", \"256\", \"512\"] {type:\"raw\"}\n",
        "BATCH_SIZE = 96 #@param [\"128\", \"96\", \"64\", \"32\", \"16\", \"8\"] {type:\"raw\"}\n",
        "LEARN_RATE = 2e-5 #@param [\"3e-4\", \"1e-4\", \"5e-5\", \"3e-5\", \"2e-5\"] {type:\"raw\"}\n",
        "EPSILON = 1e-8 #@param [\"1e-6\", \"1e-7\", \"1e-8\"] {type:\"raw\"}\n",
        "nb_warmup_steps = 0 #@param {type:\"raw\"}\n",
        "\n",
        "#@markdown ---\n",
        "CROSS_VALIDATION = True #@param {type:\"boolean\"}\n",
        "NUM_FOLDS  = 5 #@param [\"3\", \"5\", \"10\"] {type:\"raw\"}\n",
        "\n",
        "CHANGE_SPLIT = True #@param {type:\"boolean\"}\n",
        "PCTG_TRAIN = 0.75 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "PCTG_DEV   = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "PCTG_TEST  = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv_Orrg6Asl0"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwqqoNsjA0lJ"
      },
      "source": [
        "import sys, io, os, re, csv, json, string, time, datetime, random, unicodedata, itertools, collections, torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "% matplotlib inline\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "pd.set_option('precision', 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bPRlV9DA5io"
      },
      "source": [
        "models = {\n",
        "    \"english_base_cased\": ('bert-base-cased',False),\n",
        "    \"english_large_cased\": ('bert-large-cased',False),\n",
        "    \"english_base_uncased\": ('bert-base-uncased',True),\n",
        "    \"english_large_uncased\": ('bert-large-uncased',True),\n",
        "    \"multilingual\": ('bert-base-multilingual-cased',False),\n",
        "    \"finnish_cased\": ('TurkuNLP/bert-base-finnish-cased-v1',False),\n",
        "    \"finnish_uncased\": ('TurkuNLP/bert-base-finnish-uncased-v1',False),\n",
        "    \"dutch\": ('wietsedv/bert-base-dutch-cased',False),\n",
        "    \"chinese\": ('bert-base-chinese',False),\n",
        "    \"german\": ('bert-base-german-cased',False),\n",
        "    \"arabic\": ('asafaya/bert-base-arabic',False),\n",
        "    \"greek\": ('nlpaueb/bert-base-greek-uncased-v1',False),\n",
        "    \"turkish\": ('dbmdz/bert-base-turkish-cased',False)\n",
        "}\n",
        "\n",
        "datasets_olid = {\n",
        "    \"arabic\": \"https://github.com/mapama247/TFM/raw/master/ar20a.tsv\",\n",
        "    \"danish\":  \"https://github.com/mapama247/TFM/raw/master/da20a.tsv\",\n",
        "    \"english\": \"https://github.com/mapama247/TFM/raw/master/en19a.tsv\",\n",
        "    \"greek\":   \"https://github.com/mapama247/TFM/raw/master/gr20a.tsv\",\n",
        "    \"turkish\": \"https://github.com/mapama247/TFM/raw/master/tr20a.tsv\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yiOofVqA_Sp"
      },
      "source": [
        "\n",
        "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('Using', torch.cuda.get_device_name(0), ':)')\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBjy2L4sB2ps"
      },
      "source": [
        "def prepare_data(sentences, labels, random_sampling=False):\n",
        "  input_ids=[]\n",
        "  attention_masks = []\n",
        "  for sentence in sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "  \n",
        "  # Convert numpy arrays to pytorch tensors\n",
        "  inputs = torch.cat(input_ids, dim=0)\n",
        "  masks =  torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Create the DataLoader for the given set (iterator to save memory)\n",
        "  data = TensorDataset(inputs, masks, labels)\n",
        "  if random_sampling: # train data\n",
        "      sampler = RandomSampler(data)\n",
        "  else: # dev and test data\n",
        "      sampler = SequentialSampler(data)\n",
        "  dataloader = DataLoader(data, sampler=sampler, batch_size=BATCH_SIZE)\n",
        "  \n",
        "  return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHmYV39FB7Hq"
      },
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, dev_dataloader, epochs, verbose=True):\n",
        "  total_t0 = time.time() # Measure the total training time for the whole run.\n",
        "\n",
        "  training_stats = [] # training loss, validation loss, validation accuracy and timings.\n",
        "\n",
        "  for epoch_i in range(0, epochs):\n",
        "      if verbose: print('\\n======== Epoch {:} / {:} ========'.format(epoch_i+1, epochs))\n",
        "      else: print(\"Epoch\",epoch_i+1,\"of\",epochs,\"...\")\n",
        "      \n",
        "      t0 = time.time()     # Measure how long the training epoch takes.\n",
        "      total_train_loss = 0 # Reset the total loss for this epoch.\n",
        "      model.train()        # Put the model into training mode.\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "          \n",
        "          if verbose==True and step%40==0 and not step==0: # Progress update every 40 batches.\n",
        "              elapsed = format_time(time.time()-t0) # Calculate elapsed time in minutes.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader and copy each tensor to the GPU using the to() method.\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "\n",
        "          # Always clear any previously calculated gradients before performing a backward pass.\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Perform a forward pass (evaluate the model on this training batch).\n",
        "          loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=False)\n",
        "\n",
        "          # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end.\n",
        "          total_train_loss += loss.item()\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Clip the norm of the gradients to 1.0 in order to prevent the \"exploding gradients\" problem.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          optimizer.step() # Update parameters and take a step using the computed gradient.\n",
        "          scheduler.step() # Update the learning rate.\n",
        "\n",
        "      avg_train_loss = total_train_loss / len(train_dataloader) # Calculate the average loss over the training data.\n",
        "      if verbose: print(\"\\n  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "      if verbose: print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "      ########## VALIDATION ##########\n",
        "      t0 = time.time()\n",
        "      model.eval() # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
        "      eval_loss, eval_accuracy, nb_eval_examples = 0, 0, 0      # Tracking variables \n",
        "\n",
        "      # Tracking variables \n",
        "      total_eval_accuracy, total_eval_loss, nb_eval_steps = 0, 0, 0\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in dev_dataloader:\n",
        "          # Unpack the inputs from the dataloader object after adding batch to GPU\n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          \n",
        "          # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "          with torch.no_grad():        \n",
        "              # Forward pass, calculate logit predictions (output values prior to applying an activation function)\n",
        "              (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict=False)\n",
        "\n",
        "          logits = logits.detach().cpu().numpy() # Move logits to CPU\n",
        "          label_ids = b_labels.to('cpu').numpy() # Move labels to CPU\n",
        "\n",
        "          total_eval_loss += loss.item() # Accumulate the validation loss.\n",
        "          total_eval_accuracy += flat_accuracy(logits, label_ids) # Calculate the accuracy for this batch of test sentences, and accumulate it over all batches.\n",
        "\n",
        "          nb_eval_steps += 1 # Track the number of batches\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      avg_val_accuracy = total_eval_accuracy / len(dev_dataloader)\n",
        "      if verbose: print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "      # Report the average loss over all of the batches.\n",
        "      avg_val_loss = total_eval_loss / len(dev_dataloader)\n",
        "      if verbose: print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "      # Measure how long the validation run took.\n",
        "      validation_time = format_time(time.time() - t0)\n",
        "      if verbose: print(\"  Validation took: {:}\".format(validation_time))  \n",
        "\n",
        "      # Record all statistics from this epoch.\n",
        "      training_stats.append(\n",
        "          {\n",
        "              'epoch': epoch_i + 1,\n",
        "              'Train Loss': avg_train_loss,\n",
        "              'Valid Loss': avg_val_loss,\n",
        "              'Valid Acc': avg_val_accuracy,\n",
        "              'Train Time': training_time,\n",
        "              'Valid Time': validation_time\n",
        "          }\n",
        "      )\n",
        "\n",
        "  print(\"\\nTraining complete!\")\n",
        "  print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "  df_stats = pd.DataFrame(data=training_stats)\n",
        "  df_stats = df_stats.set_index('epoch')\n",
        "  return df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCaa1Tx8CGox"
      },
      "source": [
        "def predict(model, dataloader):\n",
        "  print('Predicting labels for test sentences...')\n",
        "\n",
        "  model.eval() # Put model in evaluation mode\n",
        "\n",
        "  predictions = [] # Tracking variable\n",
        "  true_labels = [] # Tracking variable\n",
        "\n",
        "  for batch in dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch) # Add batch to GPU\n",
        "    \n",
        "    b_input_ids, b_input_mask, b_labels = batch # Unpack the inputs from the dataloader\n",
        "    \n",
        "    with torch.no_grad(): # do not compute or store gradients to save memory and speed up prediction\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, return_dict=False) # Forward pass, calculate logit predictions\n",
        "    \n",
        "    logits = outputs[0] # retrieve the model outputs prior to activation\n",
        "    logits = logits.detach().cpu().numpy() # Move logits to CPU\n",
        "    label_ids = b_labels.to('cpu').numpy() # Move labels to CPU\n",
        "    \n",
        "    predictions.append(logits)    # Store predictions\n",
        "    true_labels.append(label_ids) # Store true labels\n",
        "\n",
        "  print('COMPLETED.\\n')\n",
        "  return predictions, true_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUQeeIZ4CLwq"
      },
      "source": [
        "def evaluate(predictions, true_labels, avg='macro', verbose=True):\n",
        "  avgs = ['micro', 'macro', 'weighted', 'samples']\n",
        "  if avg not in avgs:\n",
        "    raise ValueError(\"Invalid average type (avg). Expected one of: %s\" % avgs)\n",
        "\n",
        "  # Combine the predictions for each batch into a single list.\n",
        "  flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "  flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "  # Combine the correct labels for each batch into a single list.\n",
        "  flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "  # Compute the results.\n",
        "  precision = precision_score(flat_true_labels, flat_predictions, average=avg)\n",
        "  recall    = recall_score(flat_true_labels, flat_predictions, average=avg)\n",
        "  f1        = f1_score(flat_true_labels, flat_predictions, average=avg)\n",
        "  acc       = accuracy_score(flat_true_labels,flat_predictions)\n",
        "\n",
        "  # Report the results.\n",
        "  if verbose:\n",
        "    print('Accuracy:        %.4f' % acc)\n",
        "    print(avg+' Precision: %.4f' % f1)\n",
        "    print(avg+' Recall:    %.4f' % f1)\n",
        "    print(avg+' F1 score:  %.4f' % f1, \"\\n\")\n",
        "    print(confusion_matrix(flat_true_labels,flat_predictions))\n",
        "    #print(classification_report(flat_true_labels, flat_predictions, digits=2, zero_division='warn'))\n",
        "\n",
        "  return f1, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjhgVIcaCPPR"
      },
      "source": [
        "\n",
        "def plot_lengths_distribution(lengths, title=\"Length distribution of tokenized sentences\"):\n",
        "  sns.set(style=\"darkgrid\")\n",
        "  sns.set(font_scale=1.5)\n",
        "  plt.rcParams[\"figure.figsize\"]=(10,5)\n",
        "  lengths = [min(length,MAX_LEN) for length in lengths]\n",
        "  ax = sns.distplot(lengths,kde=False,rug=False, hist_kws={\"rwidth\":5,'edgecolor':'black', 'alpha':1.0}) \n",
        "  plt.title(\"Sequence length distribution\")\n",
        "  plt.xlabel(\"Sequence Length\")\n",
        "  plt.ylabel(\"Counts\")\n",
        "\n",
        "  num_truncated = lengths.count(MAX_LEN)\n",
        "  num_sentences = len(lengths)\n",
        "  print(\"{:.1%} of the training examples ({:,} of them) have more than {:,} tokens\".format(float(num_truncated)/float(num_sentences),num_truncated,MAX_LEN))\n",
        "\n",
        "def plot_value_counts(df, title=\"Label distribution\"):\n",
        "  emotions = ['TRUST','ANGRY','ANTICIP.','DISGUST','FEAR','JOY','SADNESS','SURPRISE']\n",
        "  df2 = df.replace([0,1,2,3,4,5,6,7], emotions)\n",
        "  df2.label.value_counts(normalize=False).sort_index().plot(kind='bar')\n",
        "  plt.xticks(rotation=25)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "def plot_loss(df_stats, plot_train=True, plot_valid=True):\n",
        "  sns.set(style='darkgrid') # Use plot styling from seaborn.\n",
        "  sns.set(font_scale=1.5) # Increase the plot size.\n",
        "  plt.rcParams[\"figure.figsize\"] = (12,6) # Increase the font size.\n",
        "  if plot_train: plt.plot(df_stats['Train Loss'], 'b-o', label=\"Training\")\n",
        "  if plot_valid: plt.plot(df_stats['Valid Loss'], 'g-o', label=\"Validation\")\n",
        "  plt.title(\"Training & Validation Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.xticks([1, 2, 3, 4])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnC-FqG9CVYK"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    \"\"\"Function to calculate the accuracy of our predictions vs labels\"\"\"\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    \"\"\"Function for formatting elapsed times: Takes a time in seconds and returns a string hh:mm:ss\"\"\"\n",
        "    elapsed_rounded = int(round((elapsed))) # Round to the nearest second.\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded)) # Format as hh:mm:ss\n",
        "\n",
        "def print_model_params(my_model):\n",
        "  \"\"\"Function to print all the model's parameters as a list of tuples: (name,dimensions)\"\"\"\n",
        "  params = list(my_model.named_parameters())\n",
        "  print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "  print('==== Embedding Layer ====\\n')\n",
        "  for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "  print('\\n==== First Transformer ====\\n')\n",
        "  for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "  print('\\n==== Output Layer ====\\n')\n",
        "  for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjWHQD8SCZ1K"
      },
      "source": [
        "df_train = pd.read_csv(\"NERadx.train\", delimiter='\\t', header=None, names=['label','sentence']).sample(frac=1, random_state=SEED)\n",
        "df_dev = pd.read_csv(\"NERadx.dev\"  , delimiter='\\t', header=None, names=['label','sentence']).sample(frac=1, random_state=SEED)\n",
        "df_test  = pd.read_csv(\"NERadx.test\" , delimiter='\\t', header=None, names=['label','sentence']).sample(frac=1, random_state=SEED)\n",
        "\n",
        "df_train[\"label\"].replace({8: 0}, inplace=True)\n",
        "df_dev[\"label\"].replace({8: 0}, inplace=True)\n",
        "df_test[\"label\"].replace({8: 0}, inplace=True)\n",
        "\n",
        "df_data = pd.concat([df_train, df_dev, df_test]).sample(frac=1, random_state=SEED)\n",
        "NUM_CLASSES = len(df_data.groupby('label'))\n",
        "\n",
        "if CHANGE_SPLIT:\n",
        "  if PCTG_TRAIN+PCTG_DEV+PCTG_TEST!=1:\n",
        "    raise SystemError('Check train-dev-test percentages! The sum of them should be 1.')\n",
        "  else:\n",
        "    print('The entire dataset was split using',PCTG_TRAIN,'for training,',PCTG_DEV,'for validation and',PCTG_TEST,'for testing.\\n')\n",
        "    df_train, df_dev, df_test = np.split(df_data, [int(PCTG_TRAIN*len(df_data)), int((1-PCTG_TEST)*len(df_data))])\n",
        "else:\n",
        "  print('The original train-dev-test sets are used.\\n')\n",
        "\n",
        "X_train = df_train.sentence.values\n",
        "y_train = df_train.label.values\n",
        "X_dev = df_dev.sentence.values\n",
        "y_dev = df_dev.label.values\n",
        "X_test = df_test.sentence.values\n",
        "y_test = df_test.label.values\n",
        "\n",
        "print('Number of sentences train set: {:,}'.format(len(X_train)))\n",
        "print('Number of sentences dev set:   {:,}'.format(len(X_dev)))\n",
        "print('Number of sentences test set:  {:,}'.format(len(X_test)))\n",
        "\n",
        "plot_value_counts(df_data, title=\"Value counts labels full dataset\")\n",
        "#plot_value_counts(df_train, title=\"Value counts labels train set\")\n",
        "#plot_value_counts(df_dev, title=\"Value counts labels dev set\")\n",
        "#plot_value_counts(df_test, title=\"Value counts labels test set\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO4_XjTyCt0G"
      },
      "source": [
        "\n",
        "model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path='bert-base-uncased', num_labels=NUM_CLASSES, output_attentions=False, output_hidden_states=False, return_dict=False)\n",
        "model.cuda() # Tell pytorch to run this model on the GPU.\n",
        "\n",
        "print_model_params(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJc6T1_8Cwty"
      },
      "source": [
        "bert_model = models[BERT_MODEL][0]\n",
        "lowercase = models[BERT_MODEL][1]\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=lowercase)\n",
        "print('Text will be split into tokens using the', bert_model,'built-in tokenizer.\\n')\n",
        "\n",
        "print(\"Hello I am a monkey \\U0001F648\")\n",
        "print(tokenizer.tokenize(\"Hello I am a monkey \\U0001F648\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDegcAsaC1UL"
      },
      "source": [
        "\n",
        "lengths = []\n",
        "for sent in X_train:\n",
        "  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "  l = min(MAX_LEN, len(input_ids))\n",
        "  lengths.append(l)\n",
        "\n",
        "plot_lengths_distribution(lengths, title=\"Length distribution of tokenized train sentences\")\n",
        "print('Increase MAX_LEN if too many truncated sentences, decrease it if too much padding.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0_nMWQRC4_7"
      },
      "source": [
        "train_dataloader      = prepare_data(X_train, y_train, True)\n",
        "dev_dataloader        = prepare_data(X_dev, y_dev, False)\n",
        "prediction_dataloader = prepare_data(X_test, y_test, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elEzkqbzC8Oc"
      },
      "source": [
        "adam         = AdamW(model.parameters(), lr=LEARN_RATE, eps=EPSILON)\n",
        "total_steps  = len(train_dataloader) * EPOCHS # Total number of training steps is nb_batches times nb_epochs.\n",
        "linear_sch   = get_linear_schedule_with_warmup(adam, num_warmup_steps=nb_warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "training_stats = train(model=model, optimizer=adam, train_dataloader=train_dataloader, dev_dataloader=dev_dataloader, \n",
        "                       epochs=EPOCHS, verbose=True, scheduler=linear_sch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUknVCqpC_Wz"
      },
      "source": [
        "plot_loss(training_stats)\n",
        "training_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3l-LltADC37"
      },
      "source": [
        "# if 5 folds:  20% for testing (1 fold), 70.00% for training and 10.00% for validation\n",
        "# if 10 folds: 10% for testing (1 fold), 78.75% for training and 11.25% for validation\n",
        "\n",
        "if CROSS_VALIDATION:\n",
        "  kf = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "\n",
        "  mf1s = []\n",
        "  accs = []\n",
        "  fold_num = 1\n",
        "  for train_index, test_index in kf.split(df_data):\n",
        "      print(\"##### Fold number:\", fold_num, \"#####\")\n",
        "      fold_num += 1\n",
        "      train_df = df_data.iloc[train_index]\n",
        "      test_df  = df_data.iloc[test_index]\n",
        "\n",
        "      train_df, dev_df = train_test_split(train_df, test_size=0.125) # change percentage (hyperparams?)\n",
        "\n",
        "      X_train, y_train = train_df.sentence.values, train_df.label.values\n",
        "      X_dev, y_dev = dev_df.sentence.values, dev_df.label.values\n",
        "      X_test, y_test = test_df.sentence.values, test_df.label.values\n",
        "      \n",
        "      train_dataloader      = prepare_data(X_train, y_train, 1)\n",
        "      dev_dataloader        = prepare_data(X_dev, y_dev, 0)\n",
        "      prediction_dataloader = prepare_data(X_test, y_test, 0)\n",
        "\n",
        "      training_stats = train(model, optimizer=adam, train_dataloader=train_dataloader, dev_dataloader=dev_dataloader, epochs=EPOCHS, verbose=False, scheduler=linear_sch)\n",
        "      predictions, true_labels = predict(model, prediction_dataloader)\n",
        "      mf1, acc = evaluate(predictions, true_labels, verbose=False)\n",
        "      mf1s.append(mf1)\n",
        "      accs.append(acc)\n",
        "  \n",
        "  print(\"#####################################################################\")\n",
        "  print(\"PARAMS: epochs:\",EPOCHS,\", lr_rate:\",LEARN_RATE,\"epsilon:\",EPSILON,\"...\")\n",
        "  print(\"#####################################################################\")\n",
        "  print(\"F1(CV):\",mf1s)\n",
        "  print(f\"Mean-folds-F1: {sum(mf1s)/len(mf1s)}\")\n",
        "  print(\"Acc(CV):\",accs)\n",
        "  print(f\"Mean-folds-Acc: {sum(accs)/len(accs)}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}